{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from train_channel_interval import get_batches\n",
    "from preprocess import write_video\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def put_kernels_on_grid (kernel, grid_Y, grid_X, pad = 1):\n",
    "\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x3 = tf.reshape(x2, tf.pack([grid_X, Y * grid_Y, X, channels])) #3\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x5 = tf.reshape(x4, tf.pack([1, X * grid_X, Y * grid_Y, channels])) #3\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scale to [0, 255] and convert to uint8\n",
    "    return tf.image.convert_image_dtype(x7, dtype = tf.uint8) \n",
    "\n",
    "def visualize(tensor):\n",
    "    weights = tensor.eval()\n",
    "    assert(weights.shape[0] == weights.shape[1])\n",
    "    width = weights.shape[0]\n",
    "    channels = weights.shape[2]\n",
    "    filters = weights.shape[3]\n",
    "    \n",
    "    tiling = math.ceil(math.sqrt(channels * filters))\n",
    "    output_width = tiling * width\n",
    "    output = np.zeros((output_width, output_width))\n",
    "    row, col = 0, 0\n",
    "    for i in range(channels):\n",
    "        for j in range(filters):\n",
    "            output[width*row:width*(row+1),width*col:width*(col+1)] = weights[:,:,i,j]\n",
    "            col += 1\n",
    "            if col == tiling:\n",
    "                col = 0\n",
    "                row += 1\n",
    "            \n",
    "    return output\n",
    "\n",
    "def get_summaries(tensor, name):\n",
    "    channels = tf.split(2, tensor.get_shape()[2], tensor)\n",
    "    tensors = []\n",
    "    for t in channels:\n",
    "        tensors += tf.split(3, int(t.get_shape()[3]) / 16, t)\n",
    "    \n",
    "    return [tf.image_summary(\"{}/{}\".format(name, i), put_kernels_on_grid(w, 4, 4), max_images=1) for (i,w) in enumerate(tensors)]\n",
    "\n",
    "def bias(num_units, init):\n",
    "    return tf.Variable(tf.constant(init, shape=[num_units]))\n",
    "\n",
    "def weights(shape, stddev):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "\n",
    "def conv2d(inp, b, w):\n",
    "    return tf.nn.relu(tf.nn.conv2d(inp, w, strides=[1,1,1,1], padding='SAME') + b)\n",
    "\n",
    "def pool3d(inp):\n",
    "    return tf.nn.max_pool3d(inp, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def layer(inp, num_units, filter_shape):\n",
    "    b = bias(num_units, 0.2)\n",
    "    w = weights(filter_shape + [num_units], 0.04)\n",
    "    conv = [conv2d(i, b, w) for i in inp]\n",
    "    i_pool = tf.pack(conv, axis=1)\n",
    "    o_pool = pool3d(i_pool)\n",
    "    out = tf.unpack(o_pool, axis=1)\n",
    "    return b,w,conv,i_pool,o_pool,out\n",
    "\n",
    "def dense(inp, num_in, num_out, b_init, w_init):\n",
    "    b = bias(num_out, b_init)\n",
    "    w = weights([num_in, num_out], w_init)\n",
    "    h = tf.nn.relu(tf.matmul(inp, w) + b)\n",
    "    \n",
    "    return b, w, h\n",
    "\n",
    "def dense_multi(inp, num_in, num_out, b_init, w_init):\n",
    "    b = bias(num_out, b_init)\n",
    "    w = weights([num_in, num_out], w_init)\n",
    "    unpacked = tf.unpack(inp, axis=1)\n",
    "    h = [tf.nn.relu(tf.matmul(i, w) + b) for i in unpacked]\n",
    "    return b, w, tf.pack(h, axis=1)\n",
    "\n",
    "def flatten(inp):\n",
    "    packed = tf.pack(inp, axis=1)\n",
    "    shape = packed.get_shape()[1:].num_elements()\n",
    "    reshaped = tf.reshape(packed, [-1, shape])\n",
    "    return reshaped, shape\n",
    "\n",
    "def flatten_multi(inp):\n",
    "    packed = tf.pack(inp, axis=1)\n",
    "    shape = packed.get_shape()[2:].num_elements()\n",
    "    reshaped = tf.reshape(packed, [-1, 32, shape])\n",
    "    return reshaped, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_h = tf.placeholder(tf.float32, shape=[None, 32, 64, 64, 2])\n",
    "x_m = tf.placeholder(tf.float32, shape=[None, 32, 64, 64, 2])\n",
    "\n",
    "inp_h = tf.unpack(x_h, axis=1)\n",
    "inp_m = tf.unpack(x_m, axis=1)\n",
    "y = tf.placeholder(tf.float32, shape=[None, 20])\n",
    "\n",
    "# Convolutional layers, hand\n",
    "b_conv1_h, w_conv1_h, h_conv1_h, i_pool1_h, o_pool1_h, i_conv2_h = layer(inp_h, 16, [5, 5, 2])\n",
    "b_conv2_h, w_conv2_h, h_conv2_h, i_pool2_h, o_pool2_h, i_conv3_h = layer(i_conv2_h, 32, [5, 5, 16])\n",
    "b_conv3_h, w_conv3_h, h_conv3_h, i_pool3_h, o_pool3_h, o_h = layer(i_conv3_h, 48, [4, 4, 32])\n",
    "flat_h, shape_h = flatten(o_h)\n",
    "b_fc1_h, w_fc1_h, h_fc1_h = dense(flat_h, shape_h, 256, 0.1, 0.02)\n",
    "\n",
    "# Convolutional layers, main\n",
    "b_conv1_m, w_conv1_m, h_conv1_m, i_pool1_m, o_pool1_m, i_conv2_m = layer(inp_m, 16, [5, 5, 2])\n",
    "b_conv2_m, w_conv2_m, h_conv2_m, i_pool2_m, o_pool2_m, i_conv3_m = layer(i_conv2_m, 32, [5, 5, 16])\n",
    "b_conv3_m, w_conv3_m, h_conv3_m, i_pool3_m, o_pool3_m, o_m = layer(i_conv3_m, 48, [4, 4, 32])\n",
    "flat_m, shape_m = flatten(o_m)\n",
    "b_fc1_m, w_fc1_m, h_fc1_m = dense(flat_m, shape_m, 256, 0.1, 0.02)\n",
    "\n",
    "# Fully-connected layers\n",
    "i_fc2 = tf.concat(1, [h_fc1_h, h_fc1_m])\n",
    "#b_fc1, w_fc1, h_fc1 = dense(tf.concat(1, [flat_h, flat_m]), shape_h + shape_m, 512, 0.1, 0.02)\n",
    "b_fc2 = bias(20, 0.1)\n",
    "w_fc2 = weights([512, 20], 0.02)\n",
    "h_fc2 = tf.matmul(i_fc2, w_fc2) + b_fc2\n",
    "\n",
    "# Computations\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(h_fc2, y))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(h_fc2,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHALAP = \"/media/amey/76D076A5D0766B6F/chalap\"\n",
    "MODEL = \"channel-interval\"\n",
    "\n",
    "# Create variables\n",
    "step = 0\n",
    "sess = tf.InteractiveSession()\n",
    "summary_writer = tf.train.SummaryWriter(\"{}/summaries/{}\".format(CHALAP, MODEL), sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "accuracy_summary = tf.placeholder(tf.float32, [])\n",
    "\n",
    "# Initialize/restore\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#saver.restore(sess, \"{}/checkpoints/{}-9\".format(CHALAP, MODEL))\n",
    "\n",
    "# Create summary tensors\n",
    "image_summaries = get_summaries(w_conv1_h, \"conv1\") + \\\n",
    "                  get_summaries(w_conv2_h, \"conv2\") + \\\n",
    "                  get_summaries(w_conv3_h, \"conv3\")\n",
    "accuracy_summary_op = tf.scalar_summary(\"train_accuracy\", accuracy_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.25\n",
      "step 100, training accuracy 0.0326733\n",
      "step 200, training accuracy 0.040796\n",
      "step 300, training accuracy 0.0438538\n",
      "step 400, training accuracy 0.0720698\n",
      "step 500, training accuracy 0.0774451\n",
      "step 600, training accuracy 0.0892679\n",
      "step 700, training accuracy 0.102782\n",
      "step 800, training accuracy 0.106742\n",
      "step 900, training accuracy 0.123363\n",
      "step 1000, training accuracy 0.133816\n",
      "step 1100, training accuracy 0.14119\n",
      "step 1200, training accuracy 0.158868\n",
      "step 1300, training accuracy 0.17475\n",
      "step 1400, training accuracy 0.182976\n",
      "step 1500, training accuracy 0.195769\n",
      "step 1600, training accuracy 0.203342\n",
      "step 1700, training accuracy 0.218695\n",
      "step 1800, training accuracy 0.226041\n",
      "step 1900, training accuracy 0.23627\n",
      "step 2000, training accuracy 0.249875\n",
      "step 2100, training accuracy 0.257996\n",
      "step 2200, training accuracy 0.268242\n",
      "step 2300, training accuracy 0.270535\n",
      "step 2400, training accuracy 0.28107\n",
      "step 2500, training accuracy 0.289444\n",
      "step 2600, training accuracy 0.298847\n",
      "step 2700, training accuracy 0.308404\n",
      "step 2800, training accuracy 0.318761\n",
      "step 2900, training accuracy 0.32494\n",
      "step 3000, training accuracy 0.32899\n",
      "step 3100, training accuracy 0.334118\n",
      "step 3200, training accuracy 0.342268\n",
      "Done with epoch: 0\n",
      "step 0, training accuracy 0.344496\n",
      "step 100, training accuracy 0.353809\n",
      "step 200, training accuracy 0.360221\n",
      "step 300, training accuracy 0.370232\n",
      "step 400, training accuracy 0.379348\n",
      "step 500, training accuracy 0.384494\n",
      "step 600, training accuracy 0.390689\n",
      "step 700, training accuracy 0.395323\n",
      "step 800, training accuracy 0.399628\n",
      "step 900, training accuracy 0.407475\n",
      "step 1000, training accuracy 0.412282\n",
      "step 1100, training accuracy 0.418113\n",
      "step 1200, training accuracy 0.425293\n",
      "step 1300, training accuracy 0.432554\n",
      "step 1400, training accuracy 0.437743\n",
      "step 1500, training accuracy 0.443483\n",
      "step 1600, training accuracy 0.446856\n",
      "step 1700, training accuracy 0.450649\n",
      "step 1800, training accuracy 0.453447\n",
      "step 1900, training accuracy 0.457265\n",
      "step 2000, training accuracy 0.463403\n",
      "step 2100, training accuracy 0.467867\n",
      "step 2200, training accuracy 0.472663\n",
      "step 2300, training accuracy 0.474675\n",
      "step 2400, training accuracy 0.479162\n",
      "step 2500, training accuracy 0.484618\n",
      "step 2600, training accuracy 0.490007\n",
      "step 2700, training accuracy 0.495054\n",
      "step 2800, training accuracy 0.500257\n",
      "step 2900, training accuracy 0.502886\n",
      "step 3000, training accuracy 0.505799\n",
      "step 3100, training accuracy 0.509086\n",
      "step 3200, training accuracy 0.513499\n",
      "Done with epoch: 1\n",
      "step 0, training accuracy 0.515077\n",
      "step 100, training accuracy 0.519796\n",
      "step 200, training accuracy 0.523759\n",
      "step 300, training accuracy 0.529045\n",
      "step 400, training accuracy 0.534309\n",
      "step 500, training accuracy 0.537663\n",
      "step 600, training accuracy 0.542394\n",
      "step 700, training accuracy 0.546142\n",
      "step 800, training accuracy 0.549271\n",
      "step 900, training accuracy 0.554045\n",
      "step 1000, training accuracy 0.557352\n",
      "step 1100, training accuracy 0.560969\n",
      "step 1200, training accuracy 0.564452\n",
      "step 1300, training accuracy 0.568508\n",
      "step 1400, training accuracy 0.5722\n",
      "step 1500, training accuracy 0.575493\n",
      "step 1600, training accuracy 0.578505\n",
      "step 1700, training accuracy 0.581499\n",
      "step 1800, training accuracy 0.583882\n",
      "step 1900, training accuracy 0.586704\n",
      "step 2000, training accuracy 0.590569\n",
      "step 2100, training accuracy 0.593895\n",
      "step 2200, training accuracy 0.597196\n",
      "step 2300, training accuracy 0.599441\n",
      "step 2400, training accuracy 0.602171\n",
      "step 2500, training accuracy 0.605565\n",
      "step 2600, training accuracy 0.609016\n",
      "step 2700, training accuracy 0.612038\n",
      "step 2800, training accuracy 0.615237\n",
      "step 2900, training accuracy 0.617241\n",
      "step 3000, training accuracy 0.619954\n",
      "step 3100, training accuracy 0.622672\n",
      "step 3200, training accuracy 0.625458\n",
      "Done with epoch: 2\n",
      "step 0, training accuracy 0.626593\n",
      "step 100, training accuracy 0.629556\n",
      "step 200, training accuracy 0.632202\n",
      "step 300, training accuracy 0.63545\n",
      "step 400, training accuracy 0.638525\n",
      "step 500, training accuracy 0.640583\n",
      "step 600, training accuracy 0.643505\n",
      "step 700, training accuracy 0.646168\n",
      "step 800, training accuracy 0.64869\n",
      "step 900, training accuracy 0.651392\n",
      "step 1000, training accuracy 0.653855\n",
      "step 1100, training accuracy 0.656426\n",
      "step 1200, training accuracy 0.658839\n",
      "step 1300, training accuracy 0.661555\n",
      "step 1400, training accuracy 0.664005\n",
      "step 1500, training accuracy 0.666299\n",
      "step 1600, training accuracy 0.668341\n",
      "step 1700, training accuracy 0.67057\n",
      "step 1800, training accuracy 0.67297\n",
      "step 1900, training accuracy 0.674927\n",
      "step 2000, training accuracy 0.676974\n",
      "step 2100, training accuracy 0.679148\n",
      "step 2200, training accuracy 0.681256\n",
      "step 2300, training accuracy 0.683371\n",
      "step 2400, training accuracy 0.685095\n",
      "step 2500, training accuracy 0.687439\n",
      "step 2600, training accuracy 0.689785\n",
      "step 2700, training accuracy 0.691766\n",
      "step 2800, training accuracy 0.693872\n",
      "step 2900, training accuracy 0.694817\n",
      "step 3000, training accuracy 0.696988\n",
      "step 3100, training accuracy 0.698945\n",
      "step 3200, training accuracy 0.700888\n",
      "Done with epoch: 3\n",
      "step 0, training accuracy 0.701573\n",
      "step 100, training accuracy 0.70361\n",
      "step 200, training accuracy 0.705197\n",
      "step 300, training accuracy 0.707228\n",
      "step 400, training accuracy 0.709263\n",
      "step 500, training accuracy 0.710452\n",
      "step 600, training accuracy 0.711949\n",
      "step 700, training accuracy 0.713702\n",
      "step 800, training accuracy 0.715466\n",
      "step 900, training accuracy 0.717328\n",
      "step 1000, training accuracy 0.719002\n",
      "step 1100, training accuracy 0.720723\n",
      "step 1200, training accuracy 0.722401\n",
      "step 1300, training accuracy 0.724053\n",
      "step 1400, training accuracy 0.725689\n",
      "step 1500, training accuracy 0.727278\n",
      "step 1600, training accuracy 0.728858\n",
      "step 1700, training accuracy 0.730346\n",
      "step 1800, training accuracy 0.731776\n",
      "step 1900, training accuracy 0.733038\n",
      "step 2000, training accuracy 0.734564\n",
      "step 2100, training accuracy 0.736087\n",
      "step 2200, training accuracy 0.737547\n",
      "step 2300, training accuracy 0.73881\n",
      "step 2400, training accuracy 0.740175\n",
      "step 2500, training accuracy 0.74167\n",
      "step 2600, training accuracy 0.74325\n",
      "step 2700, training accuracy 0.744774\n",
      "step 2800, training accuracy 0.746202\n",
      "step 2900, training accuracy 0.746779\n",
      "step 3000, training accuracy 0.748089\n",
      "step 3100, training accuracy 0.749404\n",
      "step 3200, training accuracy 0.750747\n",
      "Done with epoch: 4\n",
      "step 0, training accuracy 0.751216\n",
      "step 100, training accuracy 0.752687\n",
      "step 200, training accuracy 0.753883\n",
      "step 300, training accuracy 0.755189\n",
      "step 400, training accuracy 0.756571\n",
      "step 500, training accuracy 0.757656\n",
      "step 600, training accuracy 0.758947\n",
      "step 700, training accuracy 0.760204\n",
      "step 800, training accuracy 0.761494\n",
      "step 900, training accuracy 0.762598\n",
      "step 1000, training accuracy 0.763623\n",
      "step 1100, training accuracy 0.764697\n",
      "step 1200, training accuracy 0.765905\n",
      "step 1300, training accuracy 0.767036\n",
      "step 1400, training accuracy 0.768285\n",
      "step 1500, training accuracy 0.769492\n",
      "step 1600, training accuracy 0.770621\n",
      "step 1700, training accuracy 0.771818\n",
      "step 1800, training accuracy 0.773005\n",
      "step 1900, training accuracy 0.773954\n",
      "step 2000, training accuracy 0.774934\n",
      "step 2100, training accuracy 0.775985\n",
      "step 2200, training accuracy 0.777107\n",
      "step 2300, training accuracy 0.77803\n",
      "step 2400, training accuracy 0.778986\n",
      "step 2500, training accuracy 0.7801\n",
      "step 2600, training accuracy 0.781027\n",
      "step 2700, training accuracy 0.781938\n",
      "step 2800, training accuracy 0.78302\n",
      "step 2900, training accuracy 0.783468\n",
      "step 3000, training accuracy 0.784553\n",
      "step 3100, training accuracy 0.785469\n",
      "step 3200, training accuracy 0.786445\n",
      "Done with epoch: 5\n",
      "step 0, training accuracy 0.786791\n",
      "step 100, training accuracy 0.787848\n",
      "step 200, training accuracy 0.788607\n",
      "step 300, training accuracy 0.789489\n",
      "step 400, training accuracy 0.790017\n",
      "step 500, training accuracy 0.790771\n",
      "step 600, training accuracy 0.791782\n",
      "step 700, training accuracy 0.792701\n",
      "step 800, training accuracy 0.793663\n",
      "step 900, training accuracy 0.794625\n",
      "step 1000, training accuracy 0.795537\n",
      "step 1100, training accuracy 0.7963\n",
      "step 1200, training accuracy 0.797097\n",
      "step 1300, training accuracy 0.797976\n",
      "step 1400, training accuracy 0.798841\n",
      "step 1500, training accuracy 0.799761\n",
      "step 1600, training accuracy 0.800562\n",
      "step 1700, training accuracy 0.801251\n",
      "step 1800, training accuracy 0.801986\n",
      "step 1900, training accuracy 0.80274\n",
      "step 2000, training accuracy 0.803531\n",
      "step 2100, training accuracy 0.804379\n",
      "step 2200, training accuracy 0.805153\n",
      "step 2300, training accuracy 0.80591\n",
      "step 2400, training accuracy 0.806693\n",
      "step 2500, training accuracy 0.807537\n",
      "step 2600, training accuracy 0.808357\n",
      "step 2700, training accuracy 0.80912\n",
      "step 2800, training accuracy 0.809917\n",
      "step 2900, training accuracy 0.81016\n",
      "step 3000, training accuracy 0.810954\n",
      "step 3100, training accuracy 0.811741\n",
      "step 3200, training accuracy 0.812532\n",
      "Done with epoch: 6\n",
      "step 0, training accuracy 0.812803\n",
      "step 100, training accuracy 0.81351\n",
      "step 200, training accuracy 0.814151\n",
      "step 300, training accuracy 0.814881\n",
      "step 400, training accuracy 0.815628\n",
      "step 500, training accuracy 0.81622\n",
      "step 600, training accuracy 0.816914\n",
      "step 700, training accuracy 0.8176\n",
      "step 800, training accuracy 0.818246\n",
      "step 900, training accuracy 0.818955\n",
      "step 1000, training accuracy 0.819622\n",
      "step 1100, training accuracy 0.820308\n",
      "step 1200, training accuracy 0.820999\n",
      "step 1300, training accuracy 0.821737\n",
      "step 1400, training accuracy 0.822381\n",
      "step 1500, training accuracy 0.822949\n",
      "step 1600, training accuracy 0.823628\n",
      "step 1700, training accuracy 0.824314\n",
      "step 1800, training accuracy 0.824986\n",
      "step 1900, training accuracy 0.82562\n",
      "step 2000, training accuracy 0.826287\n",
      "step 2100, training accuracy 0.826967\n",
      "step 2200, training accuracy 0.827634\n",
      "step 2300, training accuracy 0.828285\n",
      "step 2400, training accuracy 0.828885\n",
      "step 2500, training accuracy 0.829518\n",
      "step 2600, training accuracy 0.830182\n",
      "step 2700, training accuracy 0.830815\n",
      "step 2800, training accuracy 0.831346\n",
      "step 2900, training accuracy 0.831666\n",
      "step 3000, training accuracy 0.83227\n",
      "step 3100, training accuracy 0.832807\n",
      "step 3200, training accuracy 0.833261\n",
      "Done with epoch: 7\n",
      "step 0, training accuracy 0.833472\n",
      "step 100, training accuracy 0.834071\n",
      "step 200, training accuracy 0.834617\n",
      "step 300, training accuracy 0.835215\n",
      "step 400, training accuracy 0.835814\n",
      "step 500, training accuracy 0.836302\n",
      "step 600, training accuracy 0.836909\n",
      "step 700, training accuracy 0.837506\n",
      "step 800, training accuracy 0.838076\n",
      "step 900, training accuracy 0.838672\n",
      "step 1000, training accuracy 0.839192\n",
      "step 1100, training accuracy 0.839561\n",
      "step 1200, training accuracy 0.840081\n",
      "step 1300, training accuracy 0.840609\n",
      "step 1400, training accuracy 0.841176\n",
      "step 1500, training accuracy 0.841707\n",
      "step 1600, training accuracy 0.842267\n",
      "step 1700, training accuracy 0.84279\n",
      "step 1800, training accuracy 0.843306\n",
      "step 1900, training accuracy 0.84382\n",
      "step 2000, training accuracy 0.844305\n",
      "step 2100, training accuracy 0.844815\n",
      "step 2200, training accuracy 0.845339\n",
      "step 2300, training accuracy 0.845881\n",
      "step 2400, training accuracy 0.84642\n",
      "step 2500, training accuracy 0.846949\n",
      "step 2600, training accuracy 0.847407\n",
      "step 2700, training accuracy 0.847842\n",
      "step 2800, training accuracy 0.848315\n",
      "step 2900, training accuracy 0.848401\n",
      "step 3000, training accuracy 0.848904\n",
      "step 3100, training accuracy 0.849344\n",
      "step 3200, training accuracy 0.849806\n",
      "Done with epoch: 8\n",
      "step 0, training accuracy 0.849971\n",
      "step 100, training accuracy 0.850435\n",
      "step 200, training accuracy 0.850916\n",
      "step 300, training accuracy 0.851417\n",
      "step 400, training accuracy 0.851915\n",
      "step 500, training accuracy 0.852392\n",
      "step 600, training accuracy 0.852839\n",
      "step 700, training accuracy 0.853306\n",
      "step 800, training accuracy 0.853775\n",
      "step 900, training accuracy 0.854247\n",
      "step 1000, training accuracy 0.854675\n",
      "step 1100, training accuracy 0.855085\n",
      "step 1200, training accuracy 0.855519\n",
      "step 1300, training accuracy 0.85588\n",
      "step 1400, training accuracy 0.856148\n",
      "step 1500, training accuracy 0.856554\n",
      "step 1600, training accuracy 0.856978\n",
      "step 1700, training accuracy 0.857431\n",
      "step 1800, training accuracy 0.857873\n",
      "step 1900, training accuracy 0.858312\n",
      "step 2000, training accuracy 0.858735\n",
      "step 2100, training accuracy 0.859178\n",
      "step 2200, training accuracy 0.859612\n",
      "step 2300, training accuracy 0.860034\n",
      "step 2400, training accuracy 0.860466\n",
      "step 2500, training accuracy 0.860865\n",
      "step 2600, training accuracy 0.861281\n",
      "step 2700, training accuracy 0.861696\n",
      "step 2800, training accuracy 0.862037\n",
      "step 2900, training accuracy 0.862138\n",
      "step 3000, training accuracy 0.862563\n",
      "step 3100, training accuracy 0.862981\n",
      "step 3200, training accuracy 0.863369\n",
      "Done with epoch: 9\n",
      "step 0, training accuracy 0.863499\n",
      "step 100, training accuracy 0.863877\n",
      "step 200, training accuracy 0.864225\n",
      "step 300, training accuracy 0.864604\n",
      "step 400, training accuracy 0.865013\n",
      "step 500, training accuracy 0.865362\n",
      "step 600, training accuracy 0.86574\n",
      "step 700, training accuracy 0.866139\n",
      "step 800, training accuracy 0.866515\n",
      "step 900, training accuracy 0.866886\n",
      "step 1000, training accuracy 0.867269\n",
      "step 1100, training accuracy 0.86763\n",
      "step 1200, training accuracy 0.868\n",
      "step 1300, training accuracy 0.868333\n",
      "step 1400, training accuracy 0.868698\n",
      "step 1500, training accuracy 0.869064\n",
      "step 1600, training accuracy 0.869391\n",
      "step 1700, training accuracy 0.869754\n",
      "step 1800, training accuracy 0.870109\n",
      "step 1900, training accuracy 0.870452\n",
      "step 2000, training accuracy 0.870827\n",
      "step 2100, training accuracy 0.871197\n",
      "step 2200, training accuracy 0.871568\n",
      "step 2300, training accuracy 0.87193\n",
      "step 2400, training accuracy 0.872267\n",
      "step 2500, training accuracy 0.872632\n",
      "step 2600, training accuracy 0.872984\n",
      "step 2700, training accuracy 0.873338\n",
      "step 2800, training accuracy 0.873677\n",
      "step 2900, training accuracy 0.873752\n",
      "step 3000, training accuracy 0.874089\n",
      "step 3100, training accuracy 0.874397\n",
      "step 3200, training accuracy 0.874683\n",
      "Done with epoch: 10\n",
      "step 0, training accuracy 0.874793\n",
      "step 100, training accuracy 0.875109\n",
      "step 200, training accuracy 0.875411\n",
      "step 300, training accuracy 0.875746\n",
      "step 400, training accuracy 0.876084\n",
      "step 500, training accuracy 0.876386\n",
      "step 600, training accuracy 0.876709\n",
      "step 700, training accuracy 0.87701\n",
      "step 800, training accuracy 0.877347\n",
      "step 900, training accuracy 0.877674\n",
      "step 1000, training accuracy 0.877987\n",
      "step 1100, training accuracy 0.878243\n",
      "step 1200, training accuracy 0.87855\n",
      "step 1300, training accuracy 0.878829\n",
      "step 1400, training accuracy 0.87912\n",
      "step 1500, training accuracy 0.879427\n",
      "step 1600, training accuracy 0.879718\n",
      "step 1700, training accuracy 0.880024\n",
      "step 1800, training accuracy 0.880326\n",
      "step 1900, training accuracy 0.880633\n",
      "step 2000, training accuracy 0.880946\n",
      "step 2100, training accuracy 0.881253\n",
      "step 2200, training accuracy 0.881547\n",
      "step 2300, training accuracy 0.881839\n",
      "step 2400, training accuracy 0.882098\n",
      "step 2500, training accuracy 0.882349\n",
      "step 2600, training accuracy 0.882577\n",
      "step 2700, training accuracy 0.88286\n",
      "step 2800, training accuracy 0.883122\n",
      "step 2900, training accuracy 0.883192\n",
      "step 3000, training accuracy 0.883477\n",
      "step 3100, training accuracy 0.883758\n",
      "step 3200, training accuracy 0.884047\n",
      "Done with epoch: 11\n",
      "step 0, training accuracy 0.884137\n",
      "step 100, training accuracy 0.884422\n",
      "step 200, training accuracy 0.884717\n",
      "step 300, training accuracy 0.885004\n",
      "step 400, training accuracy 0.885283\n",
      "step 500, training accuracy 0.885561\n",
      "step 600, training accuracy 0.885839\n",
      "step 700, training accuracy 0.886123\n",
      "step 800, training accuracy 0.88638\n",
      "step 900, training accuracy 0.886654\n",
      "step 1000, training accuracy 0.886921\n",
      "step 1100, training accuracy 0.887156\n",
      "step 1200, training accuracy 0.887332\n",
      "step 1300, training accuracy 0.887588\n",
      "step 1400, training accuracy 0.887808\n",
      "step 1500, training accuracy 0.888061\n",
      "step 1600, training accuracy 0.888301\n",
      "step 1700, training accuracy 0.888566\n",
      "step 1800, training accuracy 0.888819\n",
      "step 1900, training accuracy 0.889074\n",
      "step 2000, training accuracy 0.889335\n",
      "step 2100, training accuracy 0.889568\n",
      "step 2200, training accuracy 0.889813\n",
      "step 2300, training accuracy 0.89006\n",
      "step 2400, training accuracy 0.890317\n",
      "step 2500, training accuracy 0.890576\n",
      "step 2600, training accuracy 0.890815\n",
      "step 2700, training accuracy 0.891068\n",
      "step 2800, training accuracy 0.891297\n",
      "step 2900, training accuracy 0.891485\n",
      "step 3000, training accuracy 0.891706\n",
      "step 3100, training accuracy 0.89194\n",
      "step 3200, training accuracy 0.892183\n",
      "Done with epoch: 12\n",
      "step 0, training accuracy 0.892253\n",
      "step 100, training accuracy 0.892489\n",
      "step 200, training accuracy 0.892717\n",
      "step 300, training accuracy 0.892964\n",
      "step 400, training accuracy 0.89319\n",
      "step 500, training accuracy 0.893395\n",
      "step 600, training accuracy 0.893624\n",
      "step 700, training accuracy 0.893823\n",
      "step 800, training accuracy 0.894042\n",
      "step 900, training accuracy 0.894286\n",
      "step 1000, training accuracy 0.894512\n",
      "step 1100, training accuracy 0.894707\n",
      "step 1200, training accuracy 0.894854\n",
      "step 1300, training accuracy 0.895068\n",
      "step 1400, training accuracy 0.895287\n",
      "step 1500, training accuracy 0.895482\n",
      "step 1600, training accuracy 0.895706\n",
      "step 1700, training accuracy 0.895918\n",
      "step 1800, training accuracy 0.896127\n",
      "step 1900, training accuracy 0.896318\n",
      "step 2000, training accuracy 0.896542\n",
      "step 2100, training accuracy 0.896769\n",
      "step 2200, training accuracy 0.896978\n",
      "step 2300, training accuracy 0.897135\n",
      "step 2400, training accuracy 0.897287\n",
      "step 2500, training accuracy 0.897483\n",
      "step 2600, training accuracy 0.897691\n",
      "step 2700, training accuracy 0.897912\n",
      "step 2800, training accuracy 0.898123\n",
      "step 2900, training accuracy 0.898213\n",
      "step 3000, training accuracy 0.898424\n",
      "step 3100, training accuracy 0.898634\n",
      "step 3200, training accuracy 0.898855\n",
      "Done with epoch: 13\n",
      "step 0, training accuracy 0.898928\n",
      "step 100, training accuracy 0.899148\n",
      "step 200, training accuracy 0.899362\n",
      "step 300, training accuracy 0.899549\n",
      "step 400, training accuracy 0.899715\n",
      "step 500, training accuracy 0.899865\n",
      "step 600, training accuracy 0.900058\n",
      "step 700, training accuracy 0.900268\n",
      "step 800, training accuracy 0.900463\n",
      "step 900, training accuracy 0.900674\n",
      "step 1000, training accuracy 0.900868\n",
      "step 1100, training accuracy 0.901045\n",
      "step 1200, training accuracy 0.901255\n",
      "step 1300, training accuracy 0.901463\n",
      "step 1400, training accuracy 0.901672\n",
      "step 1500, training accuracy 0.901873\n",
      "step 1600, training accuracy 0.90207\n",
      "step 1700, training accuracy 0.902254\n",
      "step 1800, training accuracy 0.902438\n",
      "step 1900, training accuracy 0.90261\n",
      "step 2000, training accuracy 0.902803\n",
      "step 2100, training accuracy 0.902997\n",
      "step 2200, training accuracy 0.903184\n",
      "step 2300, training accuracy 0.903385\n",
      "step 2400, training accuracy 0.903578\n",
      "step 2500, training accuracy 0.903777\n",
      "step 2600, training accuracy 0.903976\n",
      "step 2700, training accuracy 0.90417\n",
      "step 2800, training accuracy 0.904367\n",
      "step 2900, training accuracy 0.90451\n",
      "step 3000, training accuracy 0.904701\n",
      "step 3100, training accuracy 0.904889\n",
      "step 3200, training accuracy 0.905084\n",
      "Done with epoch: 14\n",
      "step 0, training accuracy 0.905147\n",
      "step 100, training accuracy 0.905332\n",
      "step 200, training accuracy 0.905507\n",
      "step 300, training accuracy 0.905691\n",
      "step 400, training accuracy 0.905876\n",
      "step 500, training accuracy 0.90606\n",
      "step 600, training accuracy 0.906245\n",
      "step 700, training accuracy 0.90641\n",
      "step 800, training accuracy 0.906519\n",
      "step 900, training accuracy 0.906662\n",
      "step 1000, training accuracy 0.906813\n",
      "step 1100, training accuracy 0.906981\n",
      "step 1200, training accuracy 0.907131\n",
      "step 1300, training accuracy 0.9073\n",
      "step 1400, training accuracy 0.907469\n",
      "step 1500, training accuracy 0.907635\n",
      "step 1600, training accuracy 0.907793\n",
      "step 1700, training accuracy 0.907963\n",
      "step 1800, training accuracy 0.908137\n",
      "step 1900, training accuracy 0.908312\n",
      "step 2000, training accuracy 0.908492\n",
      "step 2100, training accuracy 0.90867\n",
      "step 2200, training accuracy 0.908843\n",
      "step 2300, training accuracy 0.909013\n",
      "step 2400, training accuracy 0.909188\n",
      "step 2500, training accuracy 0.909363\n",
      "step 2600, training accuracy 0.90954\n",
      "step 2700, training accuracy 0.909713\n",
      "step 2800, training accuracy 0.909888\n",
      "step 2900, training accuracy 0.910001\n",
      "step 3000, training accuracy 0.91017\n",
      "step 3100, training accuracy 0.910339\n",
      "step 3200, training accuracy 0.910508\n",
      "Done with epoch: 15\n",
      "step 0, training accuracy 0.910565\n",
      "step 100, training accuracy 0.910733\n",
      "step 200, training accuracy 0.910893\n",
      "step 300, training accuracy 0.911062\n",
      "step 400, training accuracy 0.911225\n",
      "step 500, training accuracy 0.911363\n",
      "step 600, training accuracy 0.911524\n",
      "step 700, training accuracy 0.911663\n",
      "step 800, training accuracy 0.911822\n",
      "step 900, training accuracy 0.911975\n",
      "step 1000, training accuracy 0.912136\n",
      "step 1100, training accuracy 0.912289\n",
      "step 1200, training accuracy 0.912436\n",
      "step 1300, training accuracy 0.912547\n",
      "step 1400, training accuracy 0.91266\n",
      "step 1500, training accuracy 0.9128\n",
      "step 1600, training accuracy 0.912923\n",
      "step 1700, training accuracy 0.913067\n",
      "step 1800, training accuracy 0.913226\n",
      "step 1900, training accuracy 0.913338\n",
      "step 2000, training accuracy 0.913442\n",
      "step 2100, training accuracy 0.913566\n",
      "step 2200, training accuracy 0.913705\n",
      "step 2300, training accuracy 0.913809\n",
      "step 2400, training accuracy 0.913953\n",
      "step 2500, training accuracy 0.9141\n",
      "step 2600, training accuracy 0.914252\n",
      "step 2700, training accuracy 0.914394\n",
      "step 2800, training accuracy 0.914542\n",
      "step 2900, training accuracy 0.914626\n",
      "step 3000, training accuracy 0.914775\n",
      "step 3100, training accuracy 0.914914\n",
      "step 3200, training accuracy 0.915066\n",
      "Done with epoch: 16\n",
      "step 0, training accuracy 0.915116\n",
      "step 100, training accuracy 0.915268\n",
      "step 200, training accuracy 0.915409\n",
      "step 300, training accuracy 0.915552\n",
      "step 400, training accuracy 0.915703\n",
      "step 500, training accuracy 0.91585\n",
      "step 600, training accuracy 0.916002\n",
      "step 700, training accuracy 0.916152\n",
      "step 800, training accuracy 0.916295\n",
      "step 900, training accuracy 0.916441\n",
      "step 1000, training accuracy 0.916583\n",
      "step 1100, training accuracy 0.916715\n",
      "step 1200, training accuracy 0.91686\n",
      "step 1300, training accuracy 0.917001\n",
      "step 1400, training accuracy 0.917139\n",
      "step 1500, training accuracy 0.917286\n",
      "step 1600, training accuracy 0.917432\n",
      "step 1700, training accuracy 0.917573\n",
      "step 1800, training accuracy 0.917696\n",
      "step 1900, training accuracy 0.917807\n",
      "step 2000, training accuracy 0.917887\n",
      "step 2100, training accuracy 0.918018\n",
      "step 2200, training accuracy 0.918159\n",
      "step 2300, training accuracy 0.918297\n",
      "step 2400, training accuracy 0.918435\n",
      "step 2500, training accuracy 0.918565\n",
      "step 2600, training accuracy 0.9187\n",
      "step 2700, training accuracy 0.91881\n",
      "step 2800, training accuracy 0.918944\n",
      "step 2900, training accuracy 0.919006\n",
      "step 3000, training accuracy 0.919144\n",
      "step 3100, training accuracy 0.919278\n",
      "step 3200, training accuracy 0.919404\n",
      "Done with epoch: 17\n",
      "step 0, training accuracy 0.919447\n",
      "step 100, training accuracy 0.919573\n",
      "step 200, training accuracy 0.919699\n",
      "step 300, training accuracy 0.919827\n",
      "step 400, training accuracy 0.91996\n",
      "step 500, training accuracy 0.920091\n",
      "step 600, training accuracy 0.920209\n",
      "step 700, training accuracy 0.920342\n",
      "step 800, training accuracy 0.920466\n",
      "step 900, training accuracy 0.920596\n",
      "step 1000, training accuracy 0.920726\n",
      "step 1100, training accuracy 0.92085\n",
      "step 1200, training accuracy 0.920983\n",
      "step 1300, training accuracy 0.921116\n",
      "step 1400, training accuracy 0.921237\n",
      "step 1500, training accuracy 0.921357\n",
      "step 1600, training accuracy 0.921478\n",
      "step 1700, training accuracy 0.921609\n",
      "step 1800, training accuracy 0.921739\n",
      "step 1900, training accuracy 0.921869\n",
      "step 2000, training accuracy 0.92198\n",
      "step 2100, training accuracy 0.922094\n",
      "step 2200, training accuracy 0.922214\n",
      "step 2300, training accuracy 0.922341\n",
      "step 2400, training accuracy 0.922466\n",
      "step 2500, training accuracy 0.922592\n",
      "step 2600, training accuracy 0.922714\n",
      "step 2700, training accuracy 0.922816\n",
      "step 2800, training accuracy 0.922935\n",
      "step 2900, training accuracy 0.923048\n",
      "step 3000, training accuracy 0.923168\n",
      "step 3100, training accuracy 0.923257\n",
      "step 3200, training accuracy 0.923349\n",
      "Done with epoch: 18\n",
      "step 0, training accuracy 0.923383\n",
      "step 100, training accuracy 0.923493\n",
      "step 200, training accuracy 0.923609\n",
      "step 300, training accuracy 0.923727\n",
      "step 400, training accuracy 0.923831\n",
      "step 500, training accuracy 0.923951\n",
      "step 600, training accuracy 0.924067\n",
      "step 700, training accuracy 0.924186\n",
      "step 800, training accuracy 0.924297\n",
      "step 900, training accuracy 0.924416\n",
      "step 1000, training accuracy 0.924526\n",
      "step 1100, training accuracy 0.92464\n",
      "step 1200, training accuracy 0.924757\n",
      "step 1300, training accuracy 0.924875\n",
      "step 1400, training accuracy 0.924993\n",
      "step 1500, training accuracy 0.925107\n",
      "step 1600, training accuracy 0.925226\n",
      "step 1700, training accuracy 0.925345\n",
      "step 1800, training accuracy 0.925456\n",
      "step 1900, training accuracy 0.92557\n",
      "step 2000, training accuracy 0.925687\n",
      "step 2100, training accuracy 0.925804\n",
      "step 2200, training accuracy 0.925902\n",
      "step 2300, training accuracy 0.926011\n",
      "step 2400, training accuracy 0.92612\n",
      "step 2500, training accuracy 0.92623\n",
      "step 2600, training accuracy 0.926339\n",
      "step 2700, training accuracy 0.926453\n",
      "step 2800, training accuracy 0.926563\n",
      "step 2900, training accuracy 0.926652\n",
      "step 3000, training accuracy 0.926755\n",
      "step 3100, training accuracy 0.92686\n",
      "step 3200, training accuracy 0.926965\n",
      "Done with epoch: 19\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    acc = []\n",
    "    batches = get_batches(batch_size, seq_len, 1, 400)\n",
    "    for i,batch in enumerate(batches):\n",
    "        step += 1\n",
    "        train_accuracy, _ = sess.run([accuracy, train_step], feed_dict={x_h:batch[0], x_m:batch[1], y: batch[2]})\n",
    "        acc.append(train_accuracy)\n",
    "        if i%100 == 0:\n",
    "            cumulative = sum(acc) / len(acc)\n",
    "            print(\"step %d, training accuracy %g\"%(i, cumulative))\n",
    "            summary_writer.add_summary(accuracy_summary_op.eval(feed_dict={accuracy_summary: cumulative}), step)\n",
    "\n",
    "    saver.save(sess, \"{}/checkpoints/{}/checkpoint\".format(CHALAP, MODEL), global_step=epoch)\n",
    "    \n",
    "    val_batches = get_batches(batch_size, seq_len, 401, 470)\n",
    "    val_cumulative = 0\n",
    "    val_acc = []\n",
    "    for j,batch in enumerate(val_batches):\n",
    "        val_accuracy = sess.run(accuracy, feed_dict={x_h:batch[0], x_m:batch[1], y: batch[2]})\n",
    "        val_acc.append(val_accuracy)\n",
    "        \n",
    "    print(\"Done with epoch: %d, validation accuracy %g\" % (epoch, sum(val_acc) / len(val_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp1 = flat.eval(feed_dict={x_h: b[0], x_m: b[1], y: b[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_fc1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(b[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [0, 0.0391089, 0.0552239, 0.0681063, 0.0798005, 0.0886228, 0.0995008, 0.109986, 0.117665, 0.1303, 0.140859, 0.144233, 0.161116, 0.173059, 0.178872, 0.188208, 0.193723, 0.206761, 0.212104, 0.221173, 0.232459, 0.240909, 0.247524, 0.247414, 0.25429, 0.261795, 0.270473, 0.280822, 0.289611, 0.294467, 0.299017, 0.304337, 0.311434, 0.313822, 0.323515, 0.329557, 0.339488, 0.347166, 0.35237, 0.357994, 0.362684, 0.368431, 0.375907, 0.379322, 0.385152, 0.39152, 0.398754, 0.403431, 0.408724, 0.413022, 0.418018, 0.422239, 0.426831, 0.433569, 0.439183, 0.444452, 0.446901, 0.451127, 0.457403, 0.463241, 0.469026, 0.474395, 0.477633, 0.48136, 0.485483, 0.489563, 0.49114, 0.496665, 0.501252, 0.50716, 0.512618, 0.515817, 0.520185, 0.523769, 0.527611, 0.532401, 0.536655, 0.541304, 0.545207, 0.549363, 0.553216, 0.557299, 0.560053, 0.563818, 0.567146, 0.570975, 0.575298, 0.579024, 0.582705, 0.585873, 0.588869, 0.592445, 0.596416, 0.599618, 0.60314, 0.605333, 0.608445, 0.611461, 0.614337, 0.615345, 0.618714, 0.621611, 0.62506, 0.628297, 0.6305, 0.633277, 0.636221, 0.639186, 0.642142, 0.645089, 0.647917, 0.650436, 0.653182, 0.655937, 0.658571, 0.660358, 0.662632, 0.665135, 0.667371, 0.669846, 0.672297, 0.674739, 0.676967, 0.679099, 0.681443, 0.683797, 0.685996, 0.688204, 0.689512, 0.691531, 0.693285, 0.695062, 0.695832, 0.69794, 0.699844, 0.701901, 0.70393, 0.705122, 0.707053, 0.708953, 0.710883, 0.712749, 0.714645, 0.716165, 0.717788, 0.719504, 0.72128, 0.722847, 0.724396, 0.725702, 0.727367, 0.728895, 0.730597, 0.732143, 0.733658, 0.735111, 0.736555, 0.738188, 0.739683, 0.741041, 0.742528, 0.743251, 0.744712, 0.745887, 0.747065, 0.747544, 0.749004, 0.75011, 0.751555, 0.753018, 0.753831, 0.755207, 0.756537, 0.757854, 0.75891, 0.760014, 0.761172, 0.762452, 0.763707, 0.765018, 0.766226, 0.767131, 0.768026, 0.769239, 0.770381, 0.771507, 0.772613, 0.773731, 0.77481, 0.775824, 0.77683, 0.77791, 0.779005, 0.780128, 0.780599, 0.781684, 0.782723, 0.783603, 0.783953, 0.784981, 0.785976, 0.787053, 0.788098, 0.788758, 0.789649, 0.790639, 0.791604, 0.792448, 0.793112, 0.793939, 0.794721, 0.795659, 0.796598, 0.797529, 0.798326, 0.799199, 0.800116, 0.800974, 0.801853, 0.802723, 0.803597, 0.804274, 0.805081, 0.805904, 0.806616, 0.807297, 0.808115, 0.808379, 0.809172, 0.809898, 0.810571, 0.810845, 0.811572, 0.812178, 0.812864, 0.81362, 0.814125, 0.814788, 0.815455, 0.816153, 0.816826, 0.817529, 0.81821, 0.818937, 0.819641, 0.820298, 0.821028, 0.821703, 0.822345, 0.823052, 0.823712, 0.824413, 0.825111, 0.825791, 0.826468, 0.827097, 0.827734, 0.828311, 0.828946, 0.829526, 0.829676, 0.83031, 0.830891, 0.831413, 0.831608, 0.83218, 0.83267, 0.833262, 0.833859, 0.834345, 0.834948, 0.835554, 0.836137, 0.836581, 0.837078, 0.837523, 0.838064, 0.83859, 0.839151, 0.839728, 0.840248, 0.840767, 0.841327, 0.841848, 0.842372, 0.842837, 0.843358, 0.843911, 0.84443, 0.84492, 0.845459, 0.845976, 0.846485, 0.846574, 0.847085, 0.84759, 0.848054, 0.848227, 0.848712, 0.849148, 0.849639, 0.850144, 0.850573, 0.851052, 0.851549, 0.852034, 0.852465, 0.852912, 0.853336, 0.853708, 0.854168, 0.854594, 0.854994, 0.855349, 0.85578, 0.856212, 0.856538, 0.856954, 0.857348, 0.857785, 0.858227, 0.858547, 0.858983, 0.859392, 0.859771, 0.86011, 0.860157, 0.860538, 0.860923, 0.86125, 0.861392, 0.861808, 0.862211, 0.862624, 0.863034, 0.863379, 0.86366, 0.864046, 0.86445, 0.864839, 0.865216, 0.865553, 0.865933, 0.866323, 0.866698, 0.867082, 0.867444, 0.867804, 0.86817, 0.868542, 0.868895, 0.869233, 0.8696, 0.869959, 0.870283, 0.870615, 0.870936, 0.871273, 0.871629, 0.871643, 0.871942, 0.87228, 0.87259, 0.872699, 0.873037, 0.873367, 0.873716, 0.874056, 0.874389, 0.874718, 0.875061, 0.875399, 0.875736, 0.876072, 0.876381, 0.876701, 0.877028, 0.87736, 0.877687, 0.878014, 0.878341, 0.878655, 0.87892, 0.879209, 0.879518, 0.879784, 0.880091, 0.880301, 0.880608, 0.880914, 0.881221, 0.881527, 0.881531, 0.881794, 0.882016, 0.882253, 0.882336, 0.88261, 0.882881, 0.88316, 0.883443, 0.88372, 0.883992, 0.884268, 0.88453, 0.884795, 0.885062, 0.88532, 0.885601, 0.885861, 0.886127, 0.886392, 0.88666, 0.886864, 0.887063, 0.887285, 0.887557, 0.887808, 0.888056, 0.88828, 0.888534, 0.888791, 0.889051, 0.88931, 0.889567, 0.889544, 0.889804, 0.890054, 0.890281, 0.890362, 0.890597, 0.890829, 0.891081, 0.891329, 0.891552, 0.891805, 0.892049, 0.892287, 0.892513, 0.892747, 0.892969, 0.893213, 0.893459, 0.893703, 0.893946, 0.894176, 0.894405, 0.894633, 0.89487, 0.895098, 0.89532, 0.895552, 0.895768, 0.895989, 0.896207, 0.896433, 0.89666, 0.896885, 0.896874, 0.897094, 0.89725, 0.897447, 0.89752, 0.897732, 0.897938, 0.898153, 0.898367, 0.89858, 0.898798, 0.899009, 0.89919, 0.899391, 0.899594, 0.899748, 0.899946, 0.900147, 0.900359, 0.900538, 0.900694, 0.900899, 0.901101, 0.901304, 0.901485, 0.901617, 0.901776, 0.901979, 0.902182, 0.902373, 0.902572, 0.902771, 0.90295, 0.902925, 0.903124, 0.903321, 0.903504, 0.903567, 0.903751, 0.903943, 0.90414, 0.904332, 0.904495, 0.904688, 0.90488, 0.905071, 0.905264, 0.905454, 0.905623, 0.905797, 0.905977, 0.906117, 0.906253, 0.906409, 0.906584, 0.906761, 0.906939, 0.907122, 0.907305, 0.907478, 0.907657, 0.907835, 0.908012, 0.908192, 0.908345, 0.908512, 0.908512, 0.908684, 0.908827, 0.90899, 0.909044, 0.909215, 0.909366, 0.90952, 0.909639, 0.909799, 0.909966, 0.910135, 0.910304, 0.91047, 0.910639, 0.910792, 0.910949, 0.911114, 0.91128, 0.911385, 0.911532, 0.911691, 0.911833, 0.911971, 0.912107, 0.912254, 0.912411, 0.912562, 0.912709, 0.912868, 0.913025, 0.913185, 0.913342, 0.913378, 0.913533, 0.913682, 0.913832, 0.913884, 0.91404, 0.914192, 0.914346, 0.914499, 0.914643, 0.914796, 0.914947, 0.915099, 0.915251, 0.915401, 0.915521, 0.91566, 0.915783, 0.915926, 0.916075, 0.916222, 0.916357, 0.91646, 0.916595, 0.916732, 0.916871, 0.91701, 0.917154, 0.917289, 0.917429, 0.917571, 0.917713, 0.917854, 0.91791, 0.918039, 0.918178, 0.918314, 0.91836, 0.918497, 0.918633, 0.918771, 0.918906, 0.919031, 0.919161, 0.919279, 0.919406, 0.919535, 0.919649, 0.919736, 0.91984, 0.919954, 0.920073, 0.920204, 0.92032, 0.920446, 0.920571, 0.920699, 0.920831, 0.920949, 0.921056, 0.92118, 0.921309, 0.921438, 0.921564, 0.921676, 0.921798, 0.921823, 0.921948, 0.922057, 0.92217, 0.92221, 0.922336, 0.922456, 0.92257, 0.922681, 0.922789, 0.92287, 0.922982, 0.92308, 0.923197, 0.923314, 0.923402, 0.923511, 0.923631, 0.923744, 0.923865, 0.923985, 0.924105, 0.924225, 0.924342, 0.924461, 0.92458, 0.924698, 0.924816, 0.924929, 0.925038, 0.925148, 0.925235, 0.925314, 0.925302, 0.925403, 0.92547, 0.925573]\n",
    "x_axis = range(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5e49ecc7b8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5eaa6a6c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_axis, temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
