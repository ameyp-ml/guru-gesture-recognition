{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from train_rnn_last import get_batches, get_pretrain_batches\n",
    "from preprocess import write_video\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def put_kernels_on_grid (kernel, grid_Y, grid_X, pad = 1):\n",
    "\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x3 = tf.reshape(x2, tf.pack([grid_X, Y * grid_Y, X, channels])) #3\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x5 = tf.reshape(x4, tf.pack([1, X * grid_X, Y * grid_Y, channels])) #3\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scale to [0, 255] and convert to uint8\n",
    "    return tf.image.convert_image_dtype(x7, dtype = tf.uint8) \n",
    "\n",
    "def visualize(tensor):\n",
    "    weights = tensor.eval()\n",
    "    assert(weights.shape[0] == weights.shape[1])\n",
    "    width = weights.shape[0]\n",
    "    channels = weights.shape[2]\n",
    "    filters = weights.shape[3]\n",
    "    \n",
    "    tiling = math.ceil(math.sqrt(channels * filters))\n",
    "    output_width = tiling * width\n",
    "    output = np.zeros((output_width, output_width))\n",
    "    row, col = 0, 0\n",
    "    for i in range(channels):\n",
    "        for j in range(filters):\n",
    "            output[width*row:width*(row+1),width*col:width*(col+1)] = weights[:,:,i,j]\n",
    "            col += 1\n",
    "            if col == tiling:\n",
    "                col = 0\n",
    "                row += 1\n",
    "            \n",
    "    return output\n",
    "\n",
    "def get_summaries(tensor, name):\n",
    "    channels = tf.split(2, tensor.get_shape()[2], tensor)\n",
    "    tensors = []\n",
    "    for t in channels:\n",
    "        tensors += tf.split(3, int(t.get_shape()[3]) / 16, t)\n",
    "    \n",
    "    return [tf.image_summary(\"{}/{}\".format(name, i), put_kernels_on_grid(w, 4, 4), max_images=1) for (i,w) in enumerate(tensors)]\n",
    "\n",
    "def extract_last_relevant(outputs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        outputs: [Tensor(batch_size, output_neurons)]: A list containing the output\n",
    "            activations of each in the batch for each time step as returned by\n",
    "            tensorflow.models.rnn.rnn.\n",
    "        length: Tensor(batch_size): The used sequence length of each example in the\n",
    "            batch with all later time steps being zeros. Should be of type tf.int32.\n",
    "\n",
    "    Returns:\n",
    "        Tensor(batch_size, output_neurons): The last relevant output activation for\n",
    "            each example in the batch.\n",
    "    \"\"\"\n",
    "    # Query shape.\n",
    "    batch_size = int(outputs.get_shape()[0])\n",
    "    max_length = int(outputs.get_shape()[1])\n",
    "    num_neurons = int(outputs.get_shape()[2])\n",
    "    # Index into flattened array as a workaround.\n",
    "    index = tf.range(0, batch_size) * max_length + (max_length - 1)\n",
    "    flat = tf.reshape(outputs, [-1, num_neurons])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant\n",
    "\n",
    "def bias(num_units, init):\n",
    "    return tf.Variable(tf.constant(init, shape=[num_units]))\n",
    "\n",
    "def weights(shape, stddev):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "\n",
    "def conv2d(inp, b, w):\n",
    "    return tf.nn.relu(tf.nn.conv2d(inp, w, strides=[1,1,1,1], padding='SAME') + b)\n",
    "\n",
    "def pool2d(inp):\n",
    "    return tf.nn.max_pool(inp, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def pool3d(inp):\n",
    "    return tf.nn.max_pool3d(inp, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def layer(inp, num_units, filter_shape):\n",
    "    num_channels = int(inp.get_shape()[4])\n",
    "    b = bias(num_units, 0.2)\n",
    "    w = weights(filter_shape + [num_channels, num_units], 0.04)\n",
    "    conv = [conv2d(i, b, w) for i in tf.unpack(inp, axis=1)]\n",
    "    out = pool3d(tf.pack(conv, axis=1))\n",
    "    return b,w,conv,out\n",
    "\n",
    "def layer2d(inp, num_units, filter_shape):\n",
    "    num_channels = int(inp.get_shape()[4])\n",
    "    b = bias(num_units, 0.2)\n",
    "    w = weights(filter_shape + [num_channels, num_units], 0.04)\n",
    "    conv = [conv2d(i, b, w) for i in tf.unpack(inp, axis=1)]\n",
    "    o_pool = tf.pack([pool2d(c) for c in conv], axis=1)\n",
    "    return b,w,conv,o_pool\n",
    "\n",
    "def dense(inp, num_in, num_out, b_init, w_init):\n",
    "    b = bias(num_out, b_init)\n",
    "    w = weights([num_in, num_out], w_init)\n",
    "    h = tf.nn.relu(tf.matmul(inp, w) + b)\n",
    "    \n",
    "    return b, w, h\n",
    "\n",
    "def dense_multi(inp, num_in, num_out, b_init, w_init):\n",
    "    b = bias(num_out, b_init)\n",
    "    w = weights([num_in, num_out], w_init)\n",
    "    unpacked = tf.unpack(inp, axis=1)\n",
    "    h = [tf.nn.relu(tf.matmul(i, w) + b) for i in unpacked]\n",
    "    return b, w, tf.pack(h, axis=1)\n",
    "\n",
    "def flatten(inp):\n",
    "    shape = inp.get_shape()[1:].num_elements()\n",
    "    reshaped = tf.reshape(inp, [-1, shape])\n",
    "    return reshaped\n",
    "\n",
    "def flatten_multi(inp):\n",
    "    seq_len = int(inp.get_shape()[1])\n",
    "    shape = inp.get_shape()[2:].num_elements()\n",
    "    reshaped = tf.reshape(inp, [-1, seq_len, shape])\n",
    "    return reshaped\n",
    "\n",
    "def lstm(inp, num_units, num_layers, batch_size, name):\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units)\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "    initial = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    out, _ = tf.nn.dynamic_rnn(\n",
    "        cell,\n",
    "        inp,\n",
    "        initial_state=initial,\n",
    "        time_major=False,\n",
    "        scope=name)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot infer num from shape (?, ?, 64, 64, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dd8838a54f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Convolutional layers, hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mb_conv1_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv1_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_conv1_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_conv2_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mb_conv2_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv2_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_conv2_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_conv3_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_conv2_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mb_conv3_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv3_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_conv3_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_conv3_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2881f0ae6696>\u001b[0m in \u001b[0;36mlayer2d\u001b[0;34m(inp, num_units, filter_shape)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mo_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amey/.pyenv/versions/3.5.2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36munpack\u001b[0;34m(value, num, axis, name)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot infer num from shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot infer num from shape (?, ?, 64, 64, 2)"
     ]
    }
   ],
   "source": [
    "seq_len = 32\n",
    "batch_size = 20\n",
    "\n",
    "x_h = tf.placeholder(tf.float32, shape=[None, seq_len, 64, 64, 2])\n",
    "x_m = tf.placeholder(tf.float32, shape=[None, seq_len, 64, 64, 2])\n",
    "y_next = tf.placeholder(tf.float32, shape=[None, 32, 32])\n",
    "#y = tf.placeholder(tf.float32, shape=[None, 20])\n",
    "\n",
    "# Convolutional layers, hand\n",
    "b_conv1_h, w_conv1_h, h_conv1_h, i_conv2_h = layer2d(x_h, 16, [5, 5])\n",
    "b_conv2_h, w_conv2_h, h_conv2_h, i_conv3_h = layer2d(i_conv2_h, 32, [5, 5])\n",
    "b_conv3_h, w_conv3_h, h_conv3_h, o_h = layer2d(i_conv3_h, 64, [4, 4])\n",
    "\n",
    "# Convolutional layers, main\n",
    "b_conv1_m, w_conv1_m, h_conv1_m, i_conv2_m = layer2d(x_m, 16, [5, 5])\n",
    "b_conv2_m, w_conv2_m, h_conv2_m, i_conv3_m = layer2d(i_conv2_m, 32, [5, 5])\n",
    "b_conv3_m, w_conv3_m, h_conv3_m, o_m = layer2d(i_conv3_m, 64, [4, 4])\n",
    "\n",
    "# Merge hand and main\n",
    "flat = flatten_multi(tf.pack([o_h, o_m], axis=2))\n",
    "\n",
    "o_lstm = lstm(flat, 1024, 2, batch_size, \"lstm\")\n",
    "last_index = int(o_lstm.get_shape()[1]) - 1\n",
    "o_lstm_last = tf.squeeze(tf.slice(o_lstm, [0, last_index, 0], [batch_size, 1, 1024]), [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = next(batches)\n",
    "p = o_lstm_last.eval(feed_dict={x_h: b[0], x_m: b[1], y_next: b[2]})\n",
    "plt.imshow(np.reshape(p[0], (2, 64, 64, 2))[1,:,:,0], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#diff_next = tf.nn.l2_normalize(tf.reshape(y_next, [-1, 64 * 64 * 4]), 1) - tf.nn.l2_normalize(o_decoder, 1)\n",
    "diff_next = o_lstm_last - tf.reshape(y_next, [-1, 1024])\n",
    "loss_next = tf.nn.l2_loss(diff_next)\n",
    "pretrain_step = tf.train.AdamOptimizer(1e-4).minimize(loss_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHALAP = \"/media/amey/76D076A5D0766B6F/chalap\"\n",
    "MODEL = \"2d-pool-pretrain\"\n",
    "\n",
    "# Create variables\n",
    "step = 0\n",
    "losses = []\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initialize/restore\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#saver.restore(sess, \"{}/checkpoints/{}-9\".format(CHALAP, MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training loss 3.28074e+08\n",
      "step 1000, training loss 1.87337e+08\n",
      "step 2000, training loss 3.86481e+08\n",
      "step 3000, training loss 1.96804e+08\n",
      "step 4000, training loss 4.04301e+08\n",
      "step 5000, training loss 3.6977e+08\n",
      "step 6000, training loss 2.74012e+08\n",
      "step 7000, training loss 2.70581e+08\n",
      "step 8000, training loss 4.24004e+08\n",
      "step 9000, training loss 3.86119e+08\n",
      "step 10000, training loss 2.71274e+08\n",
      "step 11000, training loss 1.97496e+08\n",
      "step 12000, training loss 3.74431e+08\n",
      "Done with epoch: 0\n",
      "step 0, training loss 3.24759e+08\n",
      "step 1000, training loss 1.87337e+08\n",
      "step 2000, training loss 3.86481e+08\n",
      "step 3000, training loss 1.96804e+08\n",
      "step 4000, training loss 4.04301e+08\n",
      "step 5000, training loss 3.6977e+08\n",
      "step 6000, training loss 2.74012e+08\n",
      "step 7000, training loss 2.70581e+08\n",
      "step 8000, training loss 4.24004e+08\n",
      "step 9000, training loss 3.86119e+08\n",
      "step 10000, training loss 2.71274e+08\n",
      "step 11000, training loss 1.97496e+08\n",
      "step 12000, training loss 3.74431e+08\n",
      "Done with epoch: 1\n",
      "step 0, training loss 3.24759e+08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a09ff69a8226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_h\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_m\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_next\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amey/.pyenv/versions/3.5.2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amey/.pyenv/versions/3.5.2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amey/.pyenv/versions/3.5.2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/amey/.pyenv/versions/3.5.2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amey/.pyenv/versions/3.5.2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    batches = get_pretrain_batches(batch_size, seq_len + 1)\n",
    "    for i,batch in enumerate(batches):\n",
    "        step += 1\n",
    "        l, _ = sess.run([loss_next, pretrain_step], feed_dict={x_h:batch[0], x_m:batch[1], y_next: batch[2]})\n",
    "        losses.append(l)\n",
    "        if i%1000 == 0:\n",
    "            print(\"step %d, training loss %g\"%(i, l))\n",
    "            #summary_writer.add_summary(loss_summary_op.eval(feed_dict={loss_summary: l}), step)\n",
    "\n",
    "    saver.save(sess, \"{}/checkpoints/{}\".format(CHALAP, MODEL), global_step=epoch)\n",
    "    print(\"Done with epoch: %d\" % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#b_fc1, w_fc1, h_fc1 = dense_multi(flat, int(flat.get_shape()[2]), 2048, 0.1, 0.02)\n",
    "b_fc1, w_fc1, h_fc1 = dense(flat, int(flat.get_shape()[1]), 512, 0.1, 0.02)\n",
    "\n",
    "#o_lstm = lstm(h_fc1, 512, 2, batch_size, \"lstm_h\")\n",
    "#last_index = int(o_lstm.get_shape()[1]) - 1\n",
    "#o_lstm_last = tf.squeeze(tf.slice(o_lstm, [0, last_index, 0], [batch_size, 1, 512]), [1])\n",
    "\n",
    "b_output = bias(20, 0.1)\n",
    "w_output = weights([512, 20], 0.02)\n",
    "#output = tf.matmul(o_lstm_last, w_output) + b_output\n",
    "output = tf.matmul(h_fc1, w_output) + b_output\n",
    "\n",
    "# Computations\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, y))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHALAP = \"/media/amey/76D076A5D0766B6F/chalap\"\n",
    "MODEL = \"2d-pool-pretrain\"\n",
    "\n",
    "# Create variables\n",
    "step = 0\n",
    "acc = []\n",
    "sess = tf.InteractiveSession()\n",
    "summary_writer = tf.train.SummaryWriter(\"{}/summaries\".format(CHALAP), sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "accuracy_summary = tf.placeholder(tf.float32, [])\n",
    "\n",
    "# Initialize/restore\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#saver.restore(sess, \"{}/checkpoints/{}-9\".format(CHALAP, MODEL))\n",
    "\n",
    "# Create summary tensors\n",
    "image_summaries = get_summaries(w_conv1_h, \"conv1\") + \\\n",
    "                  get_summaries(w_conv2_h, \"conv2\") + \\\n",
    "                  get_summaries(w_conv3_h, \"conv3\")\n",
    "accuracy_summary_op = tf.scalar_summary(\"train_accuracy\", accuracy_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    batches = get_batches(batch_size, seq_len)\n",
    "    for i,batch in enumerate(batches):\n",
    "        step += 1\n",
    "        train_accuracy, _ = sess.run([accuracy, train_step], feed_dict={x_h:batch[0], x_m:batch[1], y: batch[2]})\n",
    "        acc.append(train_accuracy)\n",
    "        if i%100 == 0:\n",
    "            cumulative = sum(acc) / len(acc)\n",
    "            print(\"step %d, training accuracy %g\"%(i, cumulative))\n",
    "            for summary in image_summaries:\n",
    "                summary_writer.add_summary(summary.eval(), step)\n",
    "            summary_writer.add_summary(accuracy_summary_op.eval(feed_dict={accuracy_summary: cumulative}), step)\n",
    "\n",
    "    saver.save(sess, \"{}/checkpoints/{}\".format(CHALAP, MODEL), global_step=epoch)\n",
    "    print(\"Done with epoch: %d\" % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15,  7, 18, 18])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(batches)\n",
    "np.argmax(output.eval(feed_dict={x_h: b[0], x_m: b[1], y: b[2]}), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       12, 15, 15])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5e49ecc7b8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5eaa6a6c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_axis, temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
